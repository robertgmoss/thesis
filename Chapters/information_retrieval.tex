%************************************************
\chapter{Information Retrieval}\label{ch:information_retrieval}
%************************************************
%\begin{flushleft}{\slshape    
%    At the time, Nixon was normalizing relations with China.  
%    I figured that if he could normalize relations, then so could I} \\ \medskip
%    --- Edgar Codd
%\end{flushleft}
%
\begin{itemize}
\item IR ranks documents in descending order relevance to an information need
\item expressed as a short keyword-based query~\cite{robertson:1997}
\item models define a scoring function
\item should be based on retrieval model that formalises relevance
\item Good empirical performance comes from heuristic modifications to the model
\item Most successful models all utilise a combination of common heuristics -- IDF, document length normalisation, parameters for each collection
\item We introduce an un-parameterised ranking model based purely on term frequencies
\item Concept of probabilistic document generators, and a function that estimates the probability that a given document was produced by a given generator. 
\item ``Subject-free" generator, based on the frequency of terms within a corpus
\item ``subject-based" generator, where a subject is defined by a set of key terms
\item Documents relevant to a subject are measurably more likely to have been generated by the generator for that subject than by the ``subject-free" generator
\item Rank documents according to the difference between these probability  estimates
\end{itemize}
 
\section{Ranking models}
\subsection{Vector space model}
The cosine of the angle between vectors is often used as a similarity function: 
\begin{align}
Sim(\mathbf{d}, \mathbf{q}) &= \frac{\langle\mathbf{d}, \mathbf{q} \rangle}{\|\mathbf{d}\|_2\|\mathbf{q}\|_2}\\
&= \frac{\sum_{t \in d \cup q} tf_{t, d}\cdot tf_{t, q}}{\sqrt{\sum_{t \in d} tf_{t, d}^2}\cdot\sqrt{\sum_{t \in q} tf_{t, q}^2} }
\end{align}
where $tf_{t, d}$ and $tf_{t, q}$ are the count of occurrences of $t$ in document $d$ and query $q$, known as term frequencies.  Term weights are usually adjusted by multiplying each term frequency by the Inverse Document Frequency (IDF) of the term, where IDF is calculated as 
\begin{equation}
IDF(t) = -log(df_t / N)
\end{equation} 
$N$ is the number of documents in the corpus and $df_t$ is the number of documents containing term $t$.  $IDF(t)$ is therefore the information gained by observing a document containing term $t$; if the term appears in a most documents there is very little information gained from observing it.

\subsection{Probabilistic model}
Under the probalistic model, the probability of a document being relevant given the need expressed by the query is the value of interest\cite{robertson:1997}. Binary Independence Model (BIM), achieves this by ranking documents by $P(Relevant | \mathbf{d}, \mathbf{q})$; through algebraic manipulation, the same ordering is produced by the easier calculation, 
\begin{equation}
R(\mathbf{d}, \mathbf{q}) = \sum_{t \in d \cap q} \log \frac{p_t}{1 - p_t} - \log\frac{u_t}{1 - u_t}
\end{equation}
where $p_t$ is the probability of a term $t$ appearing in a document relevant to the query, and $u_t$ the probability of $t$ appearing in a non-relevant document; that is, if $s$ is the number of relevant documents containing term $t$, and $S$ is the total number of relevant documents, then \begin{equation}
p_t = \frac{s}{S} \quad\text{and}\quad
u_t = \frac{df_t - s}{N - S}
\end{equation}  
In information theoretic terms:
\begin{equation}
\sum_t I(1 - p_t) + I(u_t) + I(1 - u_t) - I(p_t)
\end{equation}
The total information gained from observing a term $t$ is the information gained when $d$ is relevant but does not contain $t$, plus the information gained when $d$ non-relevant but contains $t$, plus the information gained when $d$ is non-relevant and does not contain $t$, minus the information gained when $d$ is relevant and contains $t$. 

In practice, these probabilities are problematic to calculate: under the assumption that the number of relevant documents is a very small proportion of the corpus, $u_t$ is estimated as $\frac{df_t}{N}$; whereas, empirically, $\frac{1}{3} + \frac{2}{3}\cdot\frac{df_t}{N}$ has been found to be a plausible estimate of $p_t$.  

The BM25 model \cite{robertson:1994} is essentially the sum of $TF \times IDF$ values, with the term frequency weighted as follows: 
\begin{equation}
BM25 = \sum_{t \in q} IDF(t) \cdot \frac{(k_1 + 1) tf_d}{k_1\alpha(b) + tf_d}\cdot \frac{(k_3 + 1)tf_q}{k_3 + tf_q} 
\end{equation}
where $k_1$ and $k_3$ are positive tuning parameters that calibrate the document and query term frequency scaling; the function
\begin{equation}
 \alpha(b) = 1 - b(1 + \frac{L_d}{L_{ave}})
\end{equation}
corresponds to document length normalisation, where $0 \leq b \leq 1$; $L_d$ is the length of $d$, and $L_{ave}$ is the average document length in the corpus. Full length normalisation is applied when $b = 1$ and none is applied when $b = 0$. 
\subsection{Language model}
Query Likelihood Language Models (LM) have a much more principled basis and are able to perform well compared to TFIDF and BM25. These assume that the query is drawn randomly according to the LM of a document and therefore seek to determine the likelihood that the query was generated given the LM of each document, i.e. 
\begin{equation}
Score(q|d) = P(q|\theta_d) 
\end{equation}
where $\theta_d$ is the LM of document $d$ \cite{hiemstra:1999, ponte:1998, crestani:1998}. 

The LM for each document is usually calculated as the maximum likelihood (ML) estimate of the probability $P(t|d)$ of a vocabulary term $t$ given the document $d$, which is simply the count of term $t$ in the document divided by the length of the $d$. In order for these models to function correctly (to avoid 0 probabilities), however, it is necessary to introduce smoothing to the ML estimates \cite{zhai:2004} thereby giving non-zero probability to unseen words. Various smoothing methods have been proposed, the most successful of these assumes a Dirichlet prior distribution over the document LM multinomials, and is therefore referred to as Dirichlet smoothing:
\begin{equation}
P(q|d) = \prod_{t \in q}\frac{tf_{t, d} + \alpha \frac{cf_t}{T}}{L_d + \alpha}
\end{equation}
%\subsection{Divergence from randomness}
%The idea of the DFR models is to infer the importance of a query term in a document by measuring the divergence of the term's distribution in the document from randomness.  In the PL2 model, the randomness is modelled by an approximation to the Poisson distribution with the use of the Laplace succession to normalise the relevance score. Using the PL2 model, the relevance score of a document d for a query $q$ is given by:
%\begin{align}
%score(d, q) = \sum_{t \in q} tw_{t, q} \cdot& \frac{1}{tfn_{t, d} + 1}\bigg(tfn_{t, d} \cdot log_2 \frac{tfn_{t, d}}{\lambda}\nonumber\\
%                             &\quad+ (\lambda - tfn_{t, d})\log_2 e + 0.5\log_2(2\pi\cdot tfn_{t, d})\bigg)
%\end{align}
%where $\lambda$ is the mean and variance of a Poisson distribution. It is given by $\lambda = F/N$. $F$ is the frequency of the query term in the collection and $N$ is the number of documents in the collection. The query term weight $tw_{t, q}$ is given by $\frac{tf_{t, q}}{\max tf_q}$.
%$tf_{t, q}$ is the query term frequency. $\max tf_q$ is the maximum query term frequency among the query terms.
%The normalized term frequency $tfn_{t, d}$ is given by the so-called normalization 2:
%\begin{equation}
%tfn_{t, d} = tf_{t, d} \cdot \log_2\left(1 + c \cdot \frac{L_{ave}}{L_d}\right)
%\end{equation}
%where $L_d$ is the document length and $L_{ave}$ is the average document length in the whole collection. $tf_{t, d}$ is the original term frequency. $c$ is the hyper-parameter of normalization 2. Its default setting is $c = 7$ for short queries and $c = 1 $for long queries [Amati and Van Rijsbergen 2002].
%\subsection{Divergence from independence}
\section{Hybrid vector-language model}
From document $d$ we construct a language model $\theta_d$ where $P(t|\theta_d)$ denotes the probability of term $t$ occurring in document $d$, given by the maximum likelihood estimate $P(t|\theta_d) = \frac{tf_{t, d}}{L_d}$. We build a language model $\theta_c$ for the entire corpus too in a similar fashion; the probability of $t$ occurring in the corpus $P(t|\theta_c) = \frac{cf_t}{T}$.

A notional document $g$ of length $n$ is constructed by making $n$ successive calls to a term generator $\G$, driven by the language model $\theta_g$, where $\G$ randomly returns a term $t$ with probability $P(t|\theta_g)$. 
%We denote the set of all documents of length $n$ deriving from generator $\G^P$ as $\G^P(n)$. 

\subsection{Probability Estimation}

Consider a similarity function over term vectors; this may be used to compare pairs of actual or generated documents, and also 
%to compare individual documents with the term vectors used to construct generators. In the latter case, 
as an estimate of the probability $P(d|\theta_g)$ that a generator $\G$ was used to produce document $d$. This will allow us to meaningfully compare a document against different generators in order to find the one most likely to have generated it.

Consider now a notional set of documents $N$, which are not relevant to \emph{any} particular subject, we will refer to these as \emph{null} documents.  The terms of such documents are modelled by the language model $\theta_c$.
%The terms of such documents are most likely to be drawn from the terms of the language with probabilities in proportion to $P_c$.

\subsection{Documents, Topics and Key Terms}
In documents relevant to one or more particular subject, each subject will be related to a set of key terms within the corpus; the relative frequency of terms appearing within each such document is greater than that of the same term within the corpus.

Such documents are thus characterisable by key terms $K = \{k_1, \ldots, k_n \}$, where each $k_i$ is a term in the corpus that acts as a keyword for the subject. The relative frequency of these terms increases over and above its relative frequency within the corpus: for example, a document about \emph{red aardvark}s may well have extra instances of that phrase, but a significantly longer document is likely to have the terms in other contexts as well.

All other terms in the document have a decreased relative frequency, although this would be a much less obvious effect especially in long documents; this is, in some sense, related to van Rijsbergen's cluster hypothesis \cite{rijsbergen:1979}. We assume terms to be distributed differently in relevant and non-relevant documents as a consequence of the existence of topics. We do not, however, assume that documents that are similar to each other have high likelihood of being relevant to the same queries. 
\subsection{Relative Probability Ranking}
For a subject described by a set of key terms $K$, any document $d$ about a subject characterised by $K$ will likely be more similar to a generated document $g$ than to a null document $n$: 
\begin{equation}
 Sim(d, g) > Sim(d, n)
\end{equation}
whereas, if document $d$ is about a different subject, with no overlapping key terms, then it is more likely to be more similar to $n$ than $g$.  As a corollary, consider the function
   \begin{equation}
       R(d,g) = Sim(d, g) -  Sim(d, n)
   \end{equation}
to be a ranking function. 
%
%\subsection{Non-key terms and noise}
For all terms in each document that are \emph{not} stated as key terms in a search, we assume that the term frequency is in ratio with the corpus term frequency. This self-same assumption is made in other probabilistic models such as the BIM model \cite{zhai:2008}. Thus, we consider notional document vectors that maintain their term frequencies for terms in the query, but otherwise effectively lose all other information. 
Our comparison to find the more relevant document has now become the higher value of $R(d_1, g)$ and $R(d_2, g)$.
%\subsection{Jensen-Shannon}

We use the statistical distances as the basis of this ranking function as follows:
\begin{equation}
SED(d,g) = 2^{JSD(d,g)} - 1
\end{equation}
where, if we recall from chapter \ref{ch:sim_structured_data}, JSD can be efficiently computed as:
\begin{equation}
JSD(d,g) = 1 - \frac{1}{2}\sum_{t \in \C} \F(P(t|\theta_d),P(t|\theta_g))
\end{equation}
and
\begin{equation}
\F(x,y) = (x+y) \log_2 (x+y) - x \log_2 x  - y \log_2 y
\end{equation}
Since they have no effect on the ordering produced by the ranking function, we remove all constants, giving
\begin{equation}
D(d,g) = -\sum_{t \in \C} \F(P(t|\theta_d),P(t|\theta_g))
\end{equation}
and $1 - D(d, g)$ as an estimate of the probability that $d$ was generated by $G$.  Thus we use the similarity function, 
\begin{equation}
Sim(d, g) = \sum_{t \in \C} \F(P(t|\theta_d),P(t|\theta_g))
\end{equation}
and the ranking function becomes
\begin{equation}
R(d, g) = \sum_{t \in K} \F(P(t|\theta_d), P(t|\theta_g)) - \F(P(t|\theta_d), P(t|\theta_c)))
\end{equation}
For each term in the query, we require only the term frequency in the corpus, from which the term frequency in the notional subject generator is calculated, and the term frequency in the document. As no global parameter such as document length is required per document, this equation is actually slightly more space-efficient than many other methods, although the run-time calculation is a little more expensive.

If an absolute  value is required, to allow comparisons of different queries, then the residual terms can also be calculated without greatly increasing the run-time cost.

\section{Retrieval performance}

\subsection{Collections and Models}
To evaluate our model against standard IR baselines we used 3 test collections from the Text REtrieval Conference (TREC), the first of which was the Text Research Collection Volumes 4 \& 5, which includes material from the Financial Times Limited (1991, 1992, 1993, 1994), the Congressional Record of the 103rd Congress (1993), the Federal Register (1994), the Foreign Broadcast Information Service (1996) and the Los Angeles Times (1989, 1990). The second was the WT10G collection, which is a general Web crawl and the third was the Blogs06 collection, a large collection of over 100,000 blogs.  For the first we used topics 301-400 from TREC-6 and TREC-7, for the second we used topics 501-550 from the TREC Web track and for the third topics 851-900 from the blog track. These collections were chosen as they are standard in IR and are quite different from one another. Having results from all 3 collections gives us insights into how generalisable a retrieval function might be.

Each topic provides three fields.  A title, a description, and a narrative; we used short queries (where only the title was used) and long queries (where all three fields were used).

All of our experiments were carried out using the Terrier IR platform~\cite{terrier}, which provides out-of-the-box implementations of our comparison metrics: TF/IDF with Robertson's TF (TFIDF), BM25, and LM with Bayesian smoothing and Dirichlet prior (DirichLM). We used default parameters for each of the models found in the literature, which are generally quite well optimised for TREC experimentation; BM25 $k_1=1.2$, $k_2=8$, $b=0.75$; Robertson TF/IDF $k_1=1.2$, $b=0.75$; Dirichlet LM $\mu=2500$. Although these values may not give strictly optimal retrieval performance they provide a fairer comparison with our parameterless function. We refer to the proposed model as Unified Probability Model (UPM) throughout.

The Terrier platform returns a ranked list of the top 1000 documents for each query with its associated score. Using supplied relevance judgements we were able to determine which of these documents were relevant.

\subsection{Results}
\begin{table}[h]
  \centering
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}}lrrrrrr}
\toprule
& \multicolumn{3}{c}{Trec 6-7} &  \multicolumn{3}{c}{Trec 6-7 Long}\\
\cmidrule(r){2-4}
\cmidrule(r){5-7}
Metric                    & mAP    & mRR    & nDCG  & mAP    & mRR   & nDCG \\
\midrule
BM25		          		 & 0.292  & 0.603  & 0.621  & 0.237  & 0.677  & 0.587 \\
DirichLM                  & 0.275  & 0.543  & 0.597  & 0.196  & 0.536  & 0.543 \\
UPM                       & 0.280  & 0.575  & 0.603  & 0.219  & 0.626  & 0.560 \\
TFIDF                     & 0.294  & 0.604  & 0.623  & 0.232  & 0.672  & 0.582 \\
\\
& \multicolumn{3}{c}{WT10G} &  \multicolumn{3}{c}{WT10G Long}\\
 \cmidrule(r){2-4} \cmidrule(r){5-7}
BM25		          		 & 0.250  & 0.584  & 0.596  & 0.184  & 0.553 & 0.522 \\
DirichLM                  & 0.249  & 0.632  & 0.597  & 0.144  & 0.460 & 0.474 \\
UPM                       & 0.189  & 0.468  & 0.539  & 0.156  & 0.486 & 0.480 \\
TFIDF                     & 0.248  & 0.578  & 0.594  & 0.178  & 0.535 & 0.512 \\
\\
& \multicolumn{3}{c}{BLOG06} &  \multicolumn{3}{c}{BLOG06 Long}\\
 \cmidrule(r){2-4} \cmidrule(r){5-7}
BM25		         		 & 0.429  & 0.695  & 0.793  & 0.362  & 0.770 & 0.747 \\
DirichLM                  & 0.457  & 0.720  & 0.802  & 0.302  & 0.729 & 0.711 \\
UPM                       & 0.318  & 0.413  & 0.713  & 0.306  & 0.687 & 0.704 \\
TFIDF                     & 0.430  & 0.695  & 0.793  & 0.356  & 0.768 & 0.740 \\
\bottomrule
\end{tabular*}
\caption{Performance of models over TREC 6 and 7, WT10G and Blogs06 collections.  First, using short queries and secondly, using long queries}
\label{tab:queries}
\end{table}

\begin{figure}[h]
\centering
%\epsfig{file=results/short/pr/Trec_6_7_pr.png,  width=3.2in}
\caption{Precision/recall, short queries, Trec6-7}
\label{fig:trecshort}
\end{figure}

\begin{figure}[h]
\centering
%\epsfig{file=results/long/pr/Trec_6_7_Long_pr.png,  width=3.2in}
\caption{Precision/recall, long queries, Trec6-7}
\label{fig:treclong}
\end{figure}

\begin{figure}[h]
\centering
%\epsfig{file=results/long/pr/WT10G_Long_pr.png,  width=3.2in}
\caption{Precision/recall, long queries, WT-10G}
\end{figure}


\begin{figure}[h]
\centering
%\epsfig{file=results/ROC_TREC_long.png,  width=3.2in}
\caption{ROC Curve, long queries, Trec6-7}
\label{fig:treclongroc}
\end{figure}

Table~\ref{tab:queries} gives an overview of the performance of all of the models over the 3 collections for both short and long queries. We use 3 standard metrics in IR to determine the relative performance of the competing models: mean average precision (mAP), mean reciprocal rank (mRR), and mean normalised discounted cumulative gain (nDCG).

Considering the short queries, the UPM model performs well on Trec6-7, clearly outperforming DirichLM and approaching the performance of the other models. Performance is worse on the WT10G collection, where the 3 other models return similar performance figures, and especially poor on the Blogs06 collection. However, when looking at the long queries we find that UPM is able to significantly outperform DirichLM for the first 2 collections and achieves similar performance for the Blogs06 collection. These results suggest that UPM is able to perform best in situations where both the documents in the collection and the queries are quite long. 

The new model performs poorly on the Blogs06 collection where the documents are generally much shorter and more variant. This is likely because the other models all include some form of document length normalisation, preventing shorter documents from being penalised. In addition to this the smoothing of terms, which is explicit in the LM and implicit in the others, is particularly important in the case of short documents and less so for longer ones. In comparison with DirichLM, UPM returns particularly good mRR scores, indicating that it is able to more consistently rank relevant documents higher in the ranked list. Figures~\ref{fig:trecshort} and \ref{fig:treclong} depict this performance difference with UPM achieving much higher precision values over the early ranks.

The ROC curve for long Trec6-7 queries (Figure~\ref{fig:treclongroc}) shows very little discrimination between all four models, indicating that each is as capable as the other at correctly classifying relevant documents. The curves for the other test collections are similar and not included for the sake of brevity%
\footnote{but will be available via URL}.

\subsection{Comparison with Base Models}

As mentioned, the ranking we have tested here is the pure form of the concept, before any heuristics or normalising factors have been applied. At time of writing these have not yet been investigated, but there is no reason to suppose that they do not exist. It is worth noting the huge improvement that has been made over the other pure forms of ranking functions through intensive investigation over, in some cases, decades. 

To check the validity of the UPM base model, we compared it against the corresponding base models of the other ranking functions before heuristics are applied. For TF-IDF this is (essentially) cosine distance; for BM25 we used the term frequency multiplied by the IDF component derived from the Binary Independence Model, and for LM we summed the relative term frequencies.

Figure \ref{fig:trecshortbase} shows the PR chart for these outcomes over the Trec6-7 short queries, which gives a taste of how much better the pure form of UPM is over the others. This is by no means the most favourable comparison; UPM still acts relatively better over the long queries, and in fact is by far the best ranking over all the query sets with the exception of short queries over the BLOG06 collection, where TF-IDF and BM25 still outperform it.

\begin{figure}[h]
\centering
%\epsfig{file=results/base/TREC6_7_BASE_PR.png,  width=3.2in}
\caption{Precision/recall base functions, short queries, Trec6-7}
\label{fig:trecshortbase}
\end{figure}


\section{Conclusions and Future Work}

In this work we have described a new retrieval model for ad hoc ranking tasks based on a probabilistic semantics and have shown that its performance is comparable to 3 highly competitive baselines. Most notably, the model was shown to outperform the frequently-used Language Model with Dirichlet smoothing in 4 out of 6 test collections. The performance of the new model is particularly strong for long queries and long documents. While the model is not able to conclusively out-perform all baselines, its performance is extremely encouraging, given that it has not been extensively adapted to the ad hoc ranking task and a number of standard IR heuristics have not yet been applied to it. This is an important point since the other ranking models have been refined over many years, particularly in order to yield good results for the TREC collection, resulting in their current form. The model is based on a simple intuitive idea and in its current form is completely unadulterated, unlike the other retrieval models which often diverge significantly from their initial theoretical base. A second key contribution of this work paper is a demonstration of how the Jensen-Shannon metric can be calculated efficiently using inverted indices, allowing it to be used for ranking tasks where data is almost always extremely sparse.

It is interesting to compare the underlying hypothesis of the UPM model with the LM model; the former is based on the probability of query terms occurring within different documents, while the latter is based directly on the probability of a query being relevant to a document. This would imply directly that UPM may be a better model over longer documents and queries, and that the LM may be better over shorter documents and queries. Our results strongly suggest that this is indeed the case and we would therefore expect that UPM would be an excellent model for tasks such as patent retrieval, where the queries are often entire documents.

In particular we believe the following to be potential avenues for future work:
\begin{itemize}
\item Introduction of common techniques used to improve IR performance. For example, investigation of the application of different values of $\epsilon$ or $\delta$, in particular where $\delta_t$ is some function of $\p(t|\C)$ as in LM smoothing functions. Furthermore the inclusion of document length normalisation may yield a significant performance improvement as it has been shown to be a key component of good IR models. We hypothesise that the lack of these adaptations in the UPM model is likely responsible for the degradation in its performance for short documents.
\item Although shown here as a ranking function, absolute values can also be calculated allowing the relevance of different queries to be compared with each other. Similarly, if similarity values are absolute then they can be used to weight terms in candidate documents for purposes such as Pseudo Relevance Feedback.
\item As the ranking is based on a similarity metric, there is a smooth progression between simple short keyword searches and a document similarity metric, achieved by increasing the number of terms being considered. The experiments we performed suggest that this model would perform well on even longer queries and may therefore be an excellent choice for tasks such as patent retrieval.
\end{itemize}