%************************************************
\chapter{Clustering, classification and outlier detection}\label{ch:clustering_classification_and_outlier_detection}
%************************************************
Cluster analysis uses a distance function to partition a vector space into groups of vectors called clusters that are mutually similar. The results of these clusters are frequently used for metric indexing.  In this chapter, we focus on two categories of algorithms to achieve this task that have a strong resemblance to indexing mechanisms used for similarity search:  agglomerative and partitional, looking at the clusterings produced with SED as the distance metric, and how the multi-way generalisation (MSED) can be used to produce clusters that are farther apart and whose elements are more mutually similar.

In the sections that follow, we devise an algorithm for hierarchical clustering based on MSED; an algorithm for selecting the seeds to the KMeans algorithm using MSED; and a cluster validity measure based on MSED.  
We show that hierarchical clusterings produced by our algorithm form more dense clusters that are farther apart than those produced by other hierarchical algorithms, using both existing cluster validity measures and our own.  
We also show that the seeds produced by our selection algorithm are more divergent than existing mechanisms employed by KMeans, although whether this actually helps the KMeans algorithm depends on the properties of the data.  Producing tight clusterings that are clearly separated is useful for metric indexing since it allows greater pruning opportunity.
\section{Clustering Techniques}
Two forms of clustering are of particular interest because of their likeness to metric indexes: agglomerative methods and partitional methods.  In this section we introduce an agglomerative algorithm, a seed selection algorithm for partitional methods and an internal cluster validity measure all of which use MSED at their core.
\subsection{Agglomerative Methods}
Agglomerative methods are bottom up, each point starts in its own cluster and pairs of clusters $(A, B)$ are merged in a greedy fashion according to some merge criterion to produce a new cluster.  The clustering ends when the required number of clusters exist or continues until all points are in the same cluster, at which point a tree can be produced showing a hierarchy of clusters.  Table \ref{tab:merge} shows the most commonly used merge criteria for hierarchical clustering.
\begin{table}
\caption{Merge criteria for different hierarchical algorithms}\label{tab:merge}
\begin{tabularx}{\textwidth}{Xlll}
\hline
method & merge criterion\\
\hline
Single-Linkage & $\argmin_{(A, B)}\{ d(a, b) : a \in A, b \in B \}$\\
Average-Linkage & $\argmin_{(A, B)}\left\lbrace\frac{1}{|A||B|} \sum_{a \in A}\sum_{b \in B} d(a, b)\right\rbrace$\\
Complete-Linkage & $\argmin_{(A, B)}\left\lbrace\max\{ d(a, b) : a \in A, b \in B \}\right\rbrace$\\
Centroid-linkage & $\argmin_{(A, B)}\left\lbrace d(c_a, c_b) : c_a =  \frac{\sum_{a \in A} a}{|A|}, c_b =  \frac{\sum_{b \in B} b}{|B|}\right\rbrace$\\
Ward's Method & $\argmin_{(A, B)}\left\lbrace d^2(c_a, c_b)\frac{|A||B|}{|A| +|B|} : c_a =  \frac{\sum_{a \in A} a}{|A|}, c_b =  \frac{\sum_{b \in B} b}{|B|}\right\rbrace$\\
\hline
\end{tabularx}
\end{table}

MSED can be used to produce another clustering criterion in the same vein:
\[
\argmin_{(A, B)} \left\lbrace MSED(A \cup B) \right\rbrace
\]
which joins the two clusters whose union has the smallest divergence.  Intuitively, this makes sense, we want to keep the clusters as tight as possible.  Given the fact that MSED correlates highly with mean intra-cluster distance, it would appear to be a faster version of Average-Linkage, but there is a subtle difference: Average-Linkage does not consider the mean distance in the union of candidate clusters, rather it considers the mean distance on the Cartesian product of the two candidate clusters.
\subsection{Partition Methods}
Partition methods divide up the space into partitions using hyperplanes.  A set of seeds are selected initially and for each seed a cluster is defined consisting of all points that are closer to that seed than any other.

The KMeans algorithm is one such method.  It begins in the usual way by selecting seeds, then partitioning the space by assigning each remaining point to its closest centroid.  For the next iteration it selects as seeds the centroids of each cluster and repartitions the space.  This process reiterates until there is no improvement according to some stopping criterion.  Here we can make use of MSED: stopping when the sum of cluster divergences does not decrease.

KMeans, however, is highly sensitive to the initial set of seeds\cite{Arthur:2007}.  Although seeds are usually chosen at random, a selection algorithm, K-Means++, aims to provide points that are far apart with a high degree of probability as shown in algorithm \ref{alg:kmeans++}.
\begin{algorithm}
\caption{KMeans++}
\label{alg:kmeans++}
\begin{algorithmic}
\State $\mathrm{C} \gets $ a uniformly random point from $X$
\While{$|\mathrm{C}| < k$}
	\State $x \gets$ a point $p$ with probability $\propto \min_{c \in C} d^2(p, c)$
	\State $\mathrm{C} \gets \mathrm{C} \cup \{x\}$
\EndWhile
\end{algorithmic}
\end{algorithm}

Seed selection is another candidate application for MSED.  Clustering is an expensive operation, and we do not want to cause unnecessary extra expense, so a greedy algorithm (algorithm \ref{alg:msed}) presents itself naturally as a fast solution that may be good enough:
\begin{algorithm}
\caption{MSED seed selection}
\label{alg:msed}
\begin{algorithmic}
\State $\mathrm{C} \gets $ a uniformly random point from $X$
\While{$|\mathrm{C}| < k$}
	\State $\mathrm{C} \gets \mathrm{C} \cup \{ arg \max_{x \in X \setminus C} MSED(C \cup \{x\}) \}$
\EndWhile
\end{algorithmic}
\end{algorithm}
Iteratively, pick a point that maximizes the divergence of the set of seeds.  From inspection, this algorithm is at least as fast as KMeans++, asymptotically; KMeans++ has to calculate the squared distance of every point to each of the $k$ seeds, while algorithm \ref{alg:msed} has to calculate the MSED value of the set of seeds together with every point for each of the $k$ seeds.
\subsection{Cluster validation}
Assessing validity of a clustering presents itself with many difficulties; the requirements, by necessity, are governed by the dataset being clustered. There are, however, two types of cluster validation technique in common usage: internal and external.  Internal measures evaluate the clustering solely on the data that was clustered.  External measures, rely on class labels produced by humans to evaluate the clustering.  

Since we are examining clustering techniques in the context of metric indexing, external measures provide little guidance with respect to our notion of a good clustering.
\subsubsection{Internal cluster measures}
The following measures produce scores that favour high similarity within cluster and low similarity between clusters. One major drawback of internal measures is that they are biased towards algorithms that use same cluster model.  Table \ref{tab:cluster_validation} shows two such commonly used measures: The Davies-Bouldin Index, which examines all pairs of clusters and produces a score based on the ratio of the sum of average distance-to-centroid and the distance between cluster centroids; lower scores are indicative of better clusterings.  The Dunn index, meanwhile, is based upon finding the minimum ratio of distance between clusters and the maximum of intra-cluster distances; here, higher scores are better.

One problem with both of these measures is that they are too slow to be of much use during clustering as, for example, a stopping criterion in partitional methods such as KMeans.  We define another validation method using MSED with much lower computational complexity:
\[
	D = \frac{1}{n} \sum_{i = 1}^n MSED(C_i)
\]
where $C_i$ is cluster $i$. This is related to the stopping criterion that we suggested be used with KMeans, however we now use the average instead of the sum to allow clusterings with different numbers of clusters to be compared.  As one might expect, lower scores are better.
\begin{table}
\caption{
Internal cluster validation metrics: $d(i, j)$ is the distance between clusters $C_i$ and $C_j$, and $d'(C_k)$ is the intra-cluster distance for cluster $C_k$; $c_i$ is the centroid for cluster $C_i$ and $\sigma_i$ is the average distance in cluster $i$ to centroid  $c_i$.
}\label{tab:cluster_validation}
\begin{tabularx}{\textwidth}{Xl}
\hline
method & score\\
\hline
Dunn index & $\min_{1 \leq i \leq n} \left\lbrace \min_{1 \leq j \leq n, i \neq j} \left\lbrace \frac{d(i, j)}{\max_{1 \leq k \leq n} d'(C_k)} \right\rbrace \right\rbrace$\\
Davies-Bouldin Index & $\frac{1}{n} \sum_{i = 1}^n \max_{i \neq j} \left( \frac{\sigma_i + \sigma_j}{d(c_i, c_j)} \right)$\\
\hline
\end{tabularx}
\end{table}

\section{Results}
To evaluate the proposed techniques, experiments were performed on synthesised data and several real world datasets.  First, we examine the cluster validation scores for the measures described in the previous section.  Secondly, we evaluate the seed selection mechanism and determine its effect on the KMeans algorithm.
\subsection{Datasets}
\subsubsection{Synthesised data}
In designing an artificial dataset, we require that the data fall into separable clusters, rather than random points with no pattern.  As such, 300 points were generated as follows: each dimension is a random gaussian in the range [number of dimensions, 100 + number of dimensions]  except for one dimension, $i$, chosen uniform-randomly in [0, 100] to bring that point into a cluster $C_i$ -- the number of clusters was therefore the same as the number of dimensions.  A number of datasets were generated with increasing dimensionality; the number of dimensions chosen respectively (and therefore cluster sizes) being 2, 4, 8, 16, 32, 64.

\subsubsection{Real world data}  It is important to consider that feature scales that are not consistent with their importance can distort results, however, SED requires normalized vectors, which is sufficient to mitigate this effect.  In total, we made use of five real world datasets, all of which were obtained from \cite{Bache:2013}.
\begin{description}
\item[Iris] Fisher \cite{fisher:1936} used this dataset to compare clustering algorithms.  It contains 150 data points each with 4 features in 3 clusters, 50 points in each cluster.  Each cluster represents one kind of Iris flower.
\item[Glass] Created by German and donated to \cite{Bache:2013} by Spiehler.  Motivated by criminological investigation since glass left at the scene of the crime can be used as evidence. It contains 214 data points each with 9 features in 6 classes of glass, distributed as follows: 70 float processed building windows, 76 non-float building windows, 17 float processed vehicle windows, 13 containers, 9 tableware, and 29 headlamps.
\item[Thyroid] First used in \cite{Coomans:1983}.  Five laboratory tests were used to try to predict a patient's thyroid class as euthyroidism, hypothyroidism or hyperthyroidism. The diagnosis (the class label) was based on a complete medical record, including anamnesis, scan etc. It contains 215 samples each with 5 features in 3 classes, distributed as follows: 150 normal, 35 hyper, 30 hypo.
\item[Wine] These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.  The analysis determined the quantities of 13 constituents found in each of the three types of wines. It was used in \cite{Aeberhard:1992}.  It contains 178 samples with 13 features in 3 classes, distributed as follows:
59 class 1,
71 class 2,
48 class 3.
\item[WDBC] Used by \cite{Mangasarian:1995}. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  They describe characteristics of the cell nuclei present in the image. It contains 569 samples each with 30 features in 2 classes, distributed as follows: 
357 benign, 
212 malignant.
\end{description}
\subsection{Validation}
\subsubsection{Davies-Bouldin Index}
Remembering that with the Davies-Bouldin metric, lower scores are better, Table \ref{tab:avg_dim} shows that on the synthesised datasets, Divergence-linkage scores the lowest.  Average-linkage, Single-linkage and Centroid-linkage score slightly higher, while  KMeans, Ward's method and Complete-linkage score significantly higher.

For the real world datasets -- Figure \ref{fig:db-index} shows the Davies-Bouldin Index score for each of the clustering mechanisms on each of these datasets -- a similar picture emerges.  Divergence-Linkage is lowest in all datasets except Iris where it is second lowest.  Average-linkage performs similarly to divergence-linkage in all cases, but has slightly higher scores -- this is no doubt a result of their similar definitions.  Single-linkage scores well with most datasets, but on the wine dataset it has an unusually high score.  KMeans and Complete-linkage score poorly with high scores on most datasets. Ward's method, too, has comparatively high scores, especially on the Glass dataset.  Centroid-linkage scores are unremarkable and lie mostly in the middle.
%\begin{figure}
%\centering
%   \includegraphics[width=0.5\textwidth]{db-index-barplot.png}
%   \caption{Davies-Bouldin Index}
%   \label{fig:db-index}
%\end{figure}%
\subsubsection{Dunn Index}
Table \ref{tab:avg_dim}, again, shows Divergence-linkage and Single-linkage performing best with the Dunn Index on synthesised data.  This time, however, Centroid-linkage also scores well.  With this measurement the difference between Divergence-linkage and Average-linkage is highlighted, with the latter scoring significantly worse.  The other clustering mechanisms, KMeans, Ward's method and Complete-linkage, in common with the Davies-Bouldin index are the least performant.  

Figure \ref{fig:dunn-index} shows the Dunn Index score on each of the real world datasets.  Divergence-linkage does not do as well on these datasets: although it does scores best on the WDBC dataset, it is worst on both Wine and Thyroid, and average for the rest.  There is no particular clear winner over all the datasets with this measure.  Average-linkage does well on WDBC and Thyroid, but badly on Wine, Glass and Iris.  Complete-linkage is average throughout, as is Ward's method, and although Centroid-linkage is best on Iris, it average for the rest.
%\begin{figure}
%\centering
%   \includegraphics[width=0.5\textwidth]{dunn-barplot.png}
%   \caption{Dunn Index}
%   \label{fig:dunn-index}
%\end{figure}%
\subsubsection{Average Divergence}
Despite being based on the same function, Divergence-linkage is outperformed by Average-linkage on the synthesised datasets, although the difference is slight as shown in Table \ref{tab:avg_dim}.  Single-linkage, again, scores well, and likewise Complete-linkage, Ward's method and KMeans do not; while Centroid-linkage is average.

Figure \ref{fig:avg-div} shows the average divergence of the clusters produced by each of the clustering mechanisms on each of the datasets.  Smaller divergences are better.  Divergence-linkage has low scores throughout.  Ward's method has the highest scores.  The Thyroid and Glass datasets are hard for all methods except Divergence-linkage and Average-linkage.  Centroid-linkage has a comparably high score for Iris.
%\begin{figure}
%\centering
%   \includegraphics[width=0.5\textwidth]{avg-div-barplot.png}
%   \caption{Average Divergence}
%   \label{fig:avg-div}
%\end{figure}%
%\begin{figure}
%        \centering
%        \begin{subfigure}[b]{0.5\textwidth}
%                \includegraphics[width=\textwidth]{db-index-barplot.png}
%                \caption{Davies-Bouldin Index --\\ lower scores are better}
%                \label{fig:db-index}
%        \end{subfigure}%
%		~         
%        \begin{subfigure}[b]{0.5\textwidth}
%                \includegraphics[width=\textwidth]{dunn-barplot.png}
%                \caption{Dunn Index --\\ higher scores are better}
%                \label{fig:dunn-index}
%        \end{subfigure}%
%		~ \\       
%        \begin{subfigure}[b]{0.5\textwidth}
%                \includegraphics[width=\textwidth]{avg-div-barplot.png}
%                \caption{Average Divergence --\\ lower scores are better}
%                \label{fig:avg-div}
%        \end{subfigure}%
%        ~        
%        \begin{subfigure}[b]{0.5\textwidth}
%                \includegraphics[width=\textwidth]{f1-barplot.png}
%                \caption{F1 Measure --\\ higher scores are better}
%                \label{fig:f1-score}
%        \end{subfigure}%
%        \caption{Cluster validity scores for different measures}\label{fig:animals}
%\end{figure}

%Only internal measures available since data is generated.  

\begin{table}
\caption{Scores for the three internal measures on the synthesised datasets.  Average and standard deviation of scores over all datasets}\label{tab:avg_dim}
\begin{tabularx}{\textwidth}{Xl|l|l}
\hline      
Algorithms						&Davies-Bouldin  &Dunn		&Avg Divergence\\
\hline 
SingleLinkage         					&$0.620 \pm 0.186$&$1.846 \pm 0.934$&$0.00077 \pm 0.00122$\\      
AverageLinkage         					&$0.611 \pm 0.225$&$1.383 \pm 0.937$&\textbf{0.00026 $\pm$ 0.00056}\\        
CompleteLinkage      					&$1.902 \pm 0.775$&$0.993 \pm 0.899$&$0.00835 \pm 0.00428$\\        
CentroidLinkage        					&$0.662 \pm 0.176$&$1.849 \pm 0.921$&$0.00177 \pm 0.00258$\\       
WardsMethod          					&$1.829 \pm 0.678$&$1.105 \pm 0.869$&$0.00555 \pm 0.00437$\\      
DivergenceLinkage      			 	&\textbf{0.581 $\pm$ 0.201}&\textbf{1.911 $\pm$ 0.932}&$0.00032 \pm 0.00071$\\ 
KMeans							&$1.764 \pm 0.632$&$1.072 \pm 0.833$&$0.00456 \pm 0.00296$\\
KMeans++							&$1.747 \pm 0.612$&$1.064 \pm 0.831$&$0.00467 \pm 0.00302$\\
KMeans-divergence					&$1.771 \pm 0.649$&$1.089 \pm 0.799$&$0.00506 \pm 0.00406$\\
\hline
\end{tabularx}
\end{table}

\begin{table}
\caption{3 classes of iris plant, 50 samples in each, 4 features.  Arguably should be 2 clusters since two overlap.}\label{tab:iris}
\begin{tabularx}{\textwidth}{l@{\hskip 1in}XXl}
\hline
Algorithms			&Davies-Bouldin  	&Dunn			&Avg Divergence\\ 
\hline
SingleLinkage			&0.409637 		&\textbf{1.901472}	&\textbf{0.000034}\\       
AverageLinkage			&0.506138        		&0.533684			&0.000092\\       
CompleteLinkage		&1.048860        		&1.239546			&0.000129\\
CentroidLinkage		&0.781545        		&1.711280			&0.000358\\       
WardsMethod			&0.730370        		&1.520542			&0.000046\\       
DivergenceLinkage		&\textbf{0.380792}	&1.786880			&0.000038\\       
KMeans				&0.732886			&1.539996			&0.000055\\
KMeans++				&0.7516854		&1.514175			&0.000057\\
KMeans-divergence		&0.7736252		&1.485374			&0.000057\\   
\hline
\end{tabularx}
\end{table}


Divergence does really well on the internal measures: although not a good as Single-linkage on Dunn and Avg Divergence, it is second best and very close.  Ward's method does well on both types of evaluation. K-Means does not benefit from more spaced out pivots on this dataset.

\begin{table}
\caption{Glass:6 classes of glass, 214 samples, distribution: 
	   70 float processed building windows,
       76 non-float building windows,
       17 float processed vehicle windows,
       13 containers,
       9 tableware
       29 headlamps;
9 features.}\label{tab:Glass}
\begin{tabularx}{\textwidth}{l@{\hskip 1in}XXl}
\hline
Algorithms			&Davies-Bouldin  	&Dunn			&Avg Divergence\\ 
\hline 
SingleLinkage			&0.489649        		&\textbf{1.858714}	&0.000160\\       
AverageLinkage			&0.390977        		&0.068432			&\textbf{0.000010}\\       
CompleteLinkage		&0.670883  		&1.691238			&0.001655\\  
CentroidLinkage		&0.601384        		&1.802741			&0.000676\\       
WardsMethod			&1.043376  		&0.728610 		&0.000692\\       
DivergenceLinkage		&\textbf{0.332704}	&0.875735			&0.000015\\       
KMeans				&1.409232			&0.3262082		&0.00074755\\
KMeans++				&1.047465			&0.7872234		&0.00102929\\
KMeans-divergence		&0.7902541		&1.188829			&0.00034769\\
\hline
\end{tabularx}
\end{table}

Divergence again scores best on Davies-Bouldin and is middle-of-the-road for external measures.  While single-linkage and average-linkage have reasonable internal scores, they score worst with external.  KMeans benefits from divergence.

\begin{table}
\caption{Thyroid: 3 classes, 215 samples, distribution:
	150 normal,
	35 hyper,
	30 hypo;
5 features.}\label{tab:Glass}
\begin{tabularx}{\textwidth}{l@{\hskip 1in}XXl}
\hline
Algorithms			&Davies-Bouldin  	&Dunn			&Avg Divergence\\ 
\hline 
SingleLinkage			&0.133134        		&5.007502			&0.001135\\       
AverageLinkage			&1.677889        		&0.325157			&0.000175\\       
CompleteLinkage		&\textbf{0.000000}	&1.325914			&0.001141\\ 
CentroidLinkage		&0.692695        		&1.838076			&0.001781\\       
WardsMethod			&\textbf{0.000000}	&0.923052 		&0.000864\\       
DivergenceLinkage		&\textbf{0.000000}	&\textbf{Infinity}	&\textbf{-0.000034}\\      
KMeans				&1.017782			&1.009915			&0.00082436\\
KMeans++				&0.9062275		&1.157552			&0.00112478\\
KMeans-divergence		&0.8428109		&1.167863			&0.00116933\\
\hline
\end{tabularx}
\end{table}

Divergence scores best on all three internal measures, performs relatively poorly on external measures, single-linkage also performs similarly.  Ward's method and complete linkage do well by both types of measure.	Divergence helps kmeans on both external and internal measures.

\begin{table}
\caption{Wine: 3 classes, 178 samples, distribution:
	59 class 1,
	71 class 2,
	48 class 3;
13 features.}\label{tab:Wine}
\begin{tabularx}{\textwidth}{l@{\hskip 1in}XXl}
\hline
Algorithms			&Davies-Bouldin	&Dunn			&Avg Divergence\\ 
\hline 
SingleLinkage			&1.015974			&0.809457			&0.000024\\       
AverageLinkage			&\textbf{0.453063}	&0.548745			&\textbf{0.000023}\\       
CompleteLinkage		&1.036393			&1.571030			&0.000066\\
CentroidLinkage		&0.751714			&\textbf{2.160639}	&0.000093\\       
WardsMethod			&0.845136			&2.102532			&0.000067\\       
DivergenceLinkage		&\textbf{0.453063}	&0.548745			&\textbf{0.000023}\\       
KMeans				&0.9270488		&1.878586			&0.00006416\\
KMeans++				&0.9132972		&1.957229			&0.00006508\\
KMeans-divergence		&0.9310699		&1.955152			&0.0000664\\
\hline
\end{tabularx}
\end{table}

Divergence and average-linkage give the same clustering and are best on two of the three internal measures.  The best all round performance come from the centroid based algorithms: centroid-linkage, ward's method and KMeans.

\begin{table}
\caption{WDBC: 2 classes, 569 samples, distribution: 
	357 benign, 
	212 malignant;
30 features}\label{tab:Wine}
\begin{tabularx}{\textwidth}{l@{\hskip 1in}XXl}
\hline
Algorithms			&Davies-Bouldin	&Dunn			&Avg Divergence\\ 
\hline
SingleLinkage			&\textbf{0.310634}	&\textbf{3.219226}     &\textbf{0.000009}\\
AverageLinkage			&0.364385			&2.744352     		&\textbf{0.000009}\\
CompleteLinkage		&1.193467			&1.500232     		&0.000025\\       
CentroidLinkage		&0.696482			&2.594994     		&0.000477\\       
WardsMethod			&0.864533			&2.128549     		&0.000033\\       
DivergenceLinkage		&\textbf{0.310634}	&\textbf{3.219226}     &\textbf{0.000009}\\       
KMeans				&1.055442			&1.809522			&0.00002371\\
KMeans++				&1.032722			&1.858522			&0.00002387\\
KMeans-divergence		&1.010494			&1.912276			&0.0000242\\
\hline
\end{tabularx}
\end{table}

Divergence and single-linkage give the same clustering and are best on all three internal measures.  KMeans does best on external measures but not so well on internal measures.

\subsection{Seed selection}
Figure \ref{fig:seeds} shows the impact of increasing the dimensionality of the space on the pivot selection algorithms.  First, the number of clusters created is the same as the number of dimensions (Figure \ref{fig:var_pivots}).  While the max divergence algorithm clearly chooses points that are highly divergent in the lower dimensions, very quickly the algorithm becomes no better than random.  Secondly, the cluster size is kept constant (Figure \ref{fig:fixed_pivots}). The effect of the dimensionality is also seen, however, much less pronounced: up to about 100 dimensions, max divergence algorithm produces significantly more divergence seeds.  
%\begin{figure}
%        \centering
%        \begin{subfigure}[b]{0.5\textwidth}
%                \includegraphics[width=\textwidth]{divergence_of_pivots.png}
%                \caption{Same number of seeds as dimensions}
%                \label{fig:var_pivots}
%        \end{subfigure}%
%        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
%          %(or a blank line to force the subfigure onto a new line)
%        \begin{subfigure}[b]{0.5\textwidth}
%                \includegraphics[width=\textwidth]{divergence_of_pivots_5_fixed.png}
%                \caption{Number of pivots is fixed at five}
%                \label{fig:fixed_pivots}
%        \end{subfigure}
%        \caption{Divergence of the seeds selected, higher divergence is better.}\label{fig:seeds}
%\end{figure}

Next, we looked at the real world datasets with a predefined number of clusters.  Table \ref{tab:cluster_validation} shows the divergence of the seeds: the max divergence algorithm again selected seeds with much higher divergence than both random and KMeans++ for all the datasets.
\begin{table}
\caption{
MSED values of the $k$ chosen seeds averaged over 100 iterations with standard deviations.  The synthesised data is the mean of averages for each of the different dimensionalities.
}\label{tab:cluster_validation}
\begin{tabularx}{\textwidth}{Xl|l|l}
\hline
Dataset			&KMeans						&KMeans++					& Max Divergence\\ 
\hline
Iris				&0.0185	$\pm$ 0.0146	&0.0323 	$\pm$ 0.0068		&\textbf{0.0509 $\pm$ 0.0037}\\
Glass			&0.0021	$\pm$ 0.0013	&0.0056	$\pm$ 0.0015		&\textbf{0.0100 $\pm$ 0.0002}\\
Thyroid			&0.0132	$\pm$ 0.0151	&0.0387	$\pm$ 0.0218		&\textbf{0.1029 $\pm$ 0.0041}\\
Wine				&0.0041	$\pm$ 0.0029	&0.0066	$\pm$ 0.0034		&\textbf{0.0171 $\pm$ 0.0016}\\
WDBC				&0.0059	$\pm$ 0.0062	&0.0092	$\pm$ 0.0064		&\textbf{0.0301 $\pm$ 0.0055}\\
\hline
synthesised		&0.0215 $\pm$ 0.0329	&0.0507 $\pm$ 0.0947		&\textbf{0.1067 $\pm$ 0.1859}\\
\hline
\end{tabularx}
\end{table}

Given that the number of seeds usually chosen for clustering algorithms, like KMeans, is usually quite low, it is clear that max divergence produces much more divergent seeds.

Figure \ref{fig:kmeans} shows how seed selection affects the final outcome of the clusterings, average across all of the real world datasets. There is little to choose between them for the external validity score, $F_1$, but there is substantial improvement in the internal measures, Davies-Bouldin and Dunn Index.   
%\begin{figure}
%\centering
%   \includegraphics[width=0.5\textwidth]{kmeans-barplot.png}
%   \caption{The effect of seed selection on KMeans}
%   \label{fig:kmeans}
%\end{figure}%

\section{Conclusion}
We have shown a number of uses for the MSED divergence function for clustering tasks.  First, Divergence-linkage clustering produces clusters that have low Davies-Bouldin Index scores compared to other clustering techniques. It also produces good scores on the Dunn index and average MSED, too.  Clusterings produced in this way are, therefore in general, more tightly packed with greater separation between clusters than other methods.    Real world data do not always form tight clusters and may not be easily separable.  It does however make a good starting point to examine further MSED based clustering for metric indexing.

Secondly, we demonstrated a selection algorithm to seed KMeans.  This algorithm consistently produced seeds that were more divergent than both random selection and KMeans++.  And although this made little difference with external validation, it improved the Davies-Bouldin and Dunn index scores for KMeans.  This makes this technique viable as a pivot selection mechanism for pivot table type metric indexes.

%2 dim: \\
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{lccc}
%\hline      
%Algorithms								&Davies-Bouldin  &Dunn		&Avg Divergence\\
%\hline 
%SingleLinkage         					&0.333457        &\textbf{3.528779}	&0.001759\\      
%AverageLinkage         					&\textbf{0.305429}&3.274081        	&\textbf{0.001398}\\        
%CompleteLinkage      					&0.693917        &2.773606        	&0.003128\\        
%CentroidLinkage        					&0.333457        &\textbf{3.528779}	&0.001759\\       
%WardsMethod          					&0.693917        &2.773606        	&0.003128\\      
%DivergenceLinkage      					&0.333457        &\textbf{3.528779}  	&0.001759\\ 
%KMeans									&0.7270627		&2.674935			&0.00352798\\
%KMeans++									&0.7316321		&2.666423			&0.00326008\\
%KMeans-divergence						&0.7437802		&2.620734			&0.0027514\\
%\hline
%\end{tabular}}
%
%4 dim: \\
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{lccc}
%\hline      
%Algorithms				&Davies-Bouldin &Dunn		   &Avg Divergence\\
%\hline 
%SingleLinkage          	&0.573570        &2.278367        &0.002807\\      
%AverageLinkage        	&0.416582        &1.140050        &0.000088\\        
%CompleteLinkage       	&1.383514        &1.048415        &0.004933\\       
%CentroidLinkage       	&0.724548        &2.296170        &0.006656\\        
%WardsMethod           	&1.349980        &1.345420        &0.000890\\      
%DivergenceLinkage      	&\textbf{0.398695}&\textbf{2.450192}&\textbf{0.000087}\\ 
%KMeans					&1.29989			&1.251359		&0.00127404\\
%KMeans++					&1.302679		&1.233783		&0.00132212\\
%KMeans-divergence		&1.241082		&1.28943			&0.0014014\\
%\hline
%\end{tabular}}
%
%8 dim: \\
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{lccc}
%\hline      
%Algorithms			&Davies-Bouldin  &Dunn			 &Avg Divergence\\
%\hline 
%SingleLinkage       	&0.536590        &1.699122        &\textbf{0.000034}\\        
%AverageLinkage      	&0.573161        &0.748057        &0.000035\\      
%CompleteLinkage     	&1.785489        &0.646783        &0.006894\\        
%CentroidLinkage     	&0.663177        &1.533783        &0.002151\\       
%WardsMethod         	&1.986503        &0.743714        &0.001909\\     
%DivergenceLinkage    &\textbf{0.523206}&\textbf{1.797036}&\textbf{0.000034}\\ 
%KMeans				&1.850432		&0.8276345		&0.00195398\\
%KMeans++				&1.856389		&0.8136799		&0.0021794\\
%KMeans-divergence	&1.840283		&0.8436102		&0.00196121\\
%\hline
%\end{tabular}}
%
%16 dim: \\
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{lccc}
%\hline       
%Algorithms					&Davies-Bouldin &Dunn		   &Avg Divergence\\
%SingleLinkage          		&0.648655        &1.323471        &\textbf{0.000017}\\      
%AverageLinkage        		&0.673565        &1.099658        &\textbf{0.000017}\\      
%CompleteLinkage       		&2.712239        &0.436791        &0.009157\\ 	       
%CentroidLinkage        		&0.650385        &\textbf{1.361624}&\textbf{0.000017}\\       
%WardsMethod           		&2.328324        &0.542942        &0.006504\\       
%DivergenceLinkage      		&\textbf{0.627898}&1.356989&\textbf{0.000017}\\ 
%KMeans						&2.29065			&0.6528458		&0.00441133\\
%KMeans++						&2.234914		&0.6505085		&0.00469317\\
%KMeans-divergence			&2.162011		&0.6689943		&0.00391865\\
%\hline
%\end{tabular}}
%
%32 dims:\\
% \resizebox{\textwidth}{!}{%
%\begin{tabular}{lccc}
%\hline      
%Algorithms				&Davies-Bouldin  &Dunn			&Avg Divergence\\
%\hline
%SingleLinkage         	&0.764334        &1.181588        &\textbf{0.000008}\\      
%AverageLinkage        	&0.802690        &1.052935        &\textbf{0.000008}\\       
%CompleteLinkage       	&2.603058        &0.497528        &0.014831\\       
%CentroidLinkage        	&0.753911        &\textbf{1.248436}        &\textbf{0.000008}\\       
%WardsMethod            	&2.469088        &0.566875        &0.008631\\         
%DivergenceLinkage       	&\textbf{0.753728}&1.225234&\textbf{0.000008}\\
%KMeans					&2.325519		&0.5556443		&0.00788282\\
%KMeans++					&2.281116		&0.549012		&0.00824063\\
%KMeans-divergence  		&2.371093		&0.5649727		&0.00954805\\
%\hline
%\end{tabular}}
%
%64 dims:\\
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{lccc}
%\hline
%Algorithms				&Davies-Bouldin  &Dunn			&Avg Divergence\\
%\hline     
%SingleLinkage          	&0.864499        &1.065082        &\textbf{0.000004}\\        
%AverageLinkage        	&0.893776        &0.980822        &\textbf{0.000004}\\       
%CompleteLinkage       	&2.234542        &0.557321        &0.011156\\       
%CentroidLinkage        	&\textbf{0.848507}&\textbf{1.124975}&\textbf{0.000004}\\       
%WardsMethod           	&2.145167        &0.659756        &0.012216\\      
%DivergenceLinkage      	&0.849264        &1.110747        &\textbf{0.000004}\\  
%KMeans					&2.090855		&0.4700952		&0.00830211\\
%KMeans++	 				&2.074613		&0.4681669		&0.00833971\\
%KMeans-divergence 		&2.270654		&0.5476863		&0.0107571\\
%\hline
%\end{tabular}}
%\section{Classification}
%\section{Outlier detection}